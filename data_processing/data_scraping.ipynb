{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing\n",
    "This notebook serves to clean the NASDAQ100 dataset to get it into a form that replicates the structure of the CHinese Stock Index datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib as joblib\n",
    "import pickle\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# from ReMASTER.system import get_data_dir\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NASDAQ100 dataset was downloaded from the following website:\n",
    "\n",
    "https://www.kaggle.com/datasets/salaheddineelkhirani/5-year-data-for-s-and-p-500-and-nasdaq-100?select=NQ_5Years_8_11_2024.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locally\n",
    "# ----------------------------------------------------------------\n",
    "# file_path = '../data/NQ_5Years_8_11_2024.csv'\n",
    "\n",
    "# # Initialize an empty DataFrame for the combined data\n",
    "# # data = pd.DataFrame()\n",
    "# data = pd.read_csv(file_path)\n",
    "\n",
    "# # Display the first few rows of the dataframe to inspect its structure\n",
    "# data.head()\n",
    "\n",
    "# on google colaboratory\n",
    "# ----------------------------------------------------------------\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Step 1: Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Step 2: Set the path to your data folder\n",
    "data_path = '/content/drive/My Drive/ECE 570/data'\n",
    "\n",
    "# data from https://www.kaggle.com/datasets/salaheddineelkhirani/5-year-data-for-s-and-p-500-and-nasdaq-100?select=NQ_5Years_8_11_2024.csv\n",
    "\n",
    "# Step 3: List files in the folder (to verify)\n",
    "print(\"Files in data folder:\")\n",
    "for filename in os.listdir(data_path):\n",
    "    print(filename)\n",
    "\n",
    "# Step 4: Load the data\n",
    "file_path = '/content/drive/My Drive/ECE 570/data/NQ_5Years_8_11_2024.csv'\n",
    "\n",
    "# Initialize an empty DataFrame for the combined data\n",
    "# data = pd.DataFrame()\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataframe to inspect its structure\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert 'Date' to datetime format\n",
    "data['Date'] = pd.to_datetime(data['Time'])\n",
    "\n",
    "# Set 'Date' as the index for time-based operations\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Verify the structure (no need to drop 'Time', as it's part of the index now)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## updated method to ensure this set fits the csi300 form with alpha158 form etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train, valid, and test sets by datetime. Earlier years are in training, recent years in test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.stats import linregress\n",
    "\n",
    "def calculate_csi300_features(df, instrument_id='NQ100'):\n",
    "    \"\"\"Calculate features and reshape data into CSI300 format.\"\"\"\n",
    "    # Make a copy of the dataframe\n",
    "    df = df.copy()\n",
    "\n",
    "    # Create features dictionary\n",
    "    features = {}\n",
    "\n",
    "    # Price Change Ratios\n",
    "    features['(close-open)/open'] = (df['Close'] - df['Open']) / df['Open']\n",
    "    features['(high-low)/open'] = (df['High'] - df['Low']) / df['Open']\n",
    "    features['(2*close-high-low)/open'] = (2 * df['Close'] - df['High'] - df['Low']) / df['Open']\n",
    "    print(\"Price Change Ratios done\")\n",
    "\n",
    "    # Moving Averages and Standard Deviations (Volatility)\n",
    "    features['Mean(close,5)/close'] = df['Close'].rolling(window=5).mean() / df['Close']\n",
    "    features['Mean(close,10)/close'] = df['Close'].rolling(window=10).mean() / df['Close']\n",
    "    features['Mean(close,20)/close'] = df['Close'].rolling(window=20).mean() / df['Close']\n",
    "    features['Std(close,5)/close'] = df['Close'].rolling(window=5).std() / df['Close']\n",
    "    features['Std(close,10)/close'] = df['Close'].rolling(window=10).std() / df['Close']\n",
    "    features['Std(close,20)/close'] = df['Close'].rolling(window=20).std() / df['Close']\n",
    "    print(\"Moving Averages and Standard Deviations done\")\n",
    "\n",
    "    # Relative Strength and Trends (Slope calculation)\n",
    "    for window in [5, 10, 20]:\n",
    "        features[f'Slope(close,{window})/close'] = df['Close'].rolling(window).apply(\n",
    "            lambda x: linregress(range(len(x)), x).slope / x[-1] if len(x) == window else np.nan\n",
    "        )\n",
    "    print(\"Relative Strength and Trends done\")\n",
    "\n",
    "    # Volume-Weighted Measures\n",
    "    features['VWAP/close'] = (df['Volume'] * df['Close']).cumsum() / df['Volume'].cumsum() / df['Close']\n",
    "    features['Mean(volume,5)/(volume+1e-12)'] = df['Volume'].rolling(window=5).mean() / (df['Volume'] + 1e-12)\n",
    "    features['Std(volume,5)/(volume+1e-12)'] = df['Volume'].rolling(window=5).std() / (df['Volume'] + 1e-12)\n",
    "    print(\"Volume-Weighted Measures done\")\n",
    "\n",
    "    # Historical Close Ratios\n",
    "    features['Ref(close,5)/close'] = df['Close'].shift(5) / df['Close']\n",
    "    features['Ref(close,10)/close'] = df['Close'].shift(10) / df['Close']\n",
    "    features['Ref(close,20)/close'] = df['Close'].shift(20) / df['Close']\n",
    "    print(\"Historical Close Ratios done\")\n",
    "\n",
    "    # Correlation Measures\n",
    "    features['Corr(close,log(volume+1),5)'] = df['Close'].rolling(window=5).corr(np.log(df['Volume'] + 1))\n",
    "    features['Corr(close/Ref(close,1),log(volume/Ref(volume,1)+1),5)'] = (\n",
    "        (df['Close'] / df['Close'].shift(1)).rolling(window=5)\n",
    "        .corr(np.log(df['Volume'] / df['Volume'].shift(1) + 1))\n",
    "    )\n",
    "    print(\"Correlation Measures done\")\n",
    "\n",
    "    # Price Extremes and Ratios\n",
    "    features['Max(high,5)/close'] = df['High'].rolling(window=5).max() / df['Close']\n",
    "    features['Min(low,5)/close'] = df['Low'].rolling(window=5).min() / df['Close']\n",
    "    print(\"Price Extremes and Ratios done\")\n",
    "\n",
    "    # Create DataFrame from features with the same index as input data\n",
    "    feature_df = pd.DataFrame(features, index=df.index)\n",
    "\n",
    "    # Add datetime (using the index date) and instrument columns\n",
    "    feature_df['datetime'] = feature_df.index.date\n",
    "    feature_df['instrument'] = instrument_id\n",
    "\n",
    "    # Reorder columns to put datetime and instrument first\n",
    "    cols = ['datetime', 'instrument'] + [col for col in feature_df.columns if col not in ['datetime', 'instrument']]\n",
    "    feature_df = feature_df[cols]\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "def process_single_instrument(data, instrument_id, date_ranges):\n",
    "    \"\"\"Process data for a single instrument.\"\"\"\n",
    "    # Calculate features\n",
    "    processed_df = calculate_csi300_features(data, instrument_id)\n",
    "\n",
    "    # Set datetime and instrument as index\n",
    "    processed_df.set_index(['datetime', 'instrument'], inplace=True)\n",
    "\n",
    "    # Split data based on date ranges\n",
    "    train_data = processed_df[processed_df.index.get_level_values(0) <= date_ranges['train_end'].date()]\n",
    "    valid_data = processed_df[\n",
    "        (processed_df.index.get_level_values(0) >= date_ranges['valid_start'].date()) &\n",
    "        (processed_df.index.get_level_values(0) <= date_ranges['valid_end'].date())\n",
    "    ]\n",
    "    test_data = processed_df[\n",
    "        (processed_df.index.get_level_values(0) >= date_ranges['test_start'].date()) &\n",
    "        (processed_df.index.get_level_values(0) <= date_ranges['test_end'].date())\n",
    "    ]\n",
    "\n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "# Debug: Print input data structure\n",
    "print(\"Input data structure:\")\n",
    "print(data.head())\n",
    "print(\"\\nInput data info:\")\n",
    "print(data.info())\n",
    "\n",
    "# Define date ranges\n",
    "date_ranges = {\n",
    "    'train_end': datetime(2022, 3, 31),\n",
    "    'valid_start': datetime(2022, 4, 1),\n",
    "    'valid_end': datetime(2022, 6, 30),\n",
    "    'test_start': datetime(2022, 7, 1),\n",
    "    'test_end': datetime(2024, 12, 31)\n",
    "}\n",
    "\n",
    "# Process the data for NQ100\n",
    "instrument_id = 'NQ100'\n",
    "train_data, valid_data, test_data = process_single_instrument(data, instrument_id, date_ranges)\n",
    "\n",
    "# Save the processed datasets\n",
    "train_data.to_csv('train_data_2020_2022_NQ100.csv')\n",
    "valid_data.to_csv('valid_data_2022_NQ100.csv')\n",
    "test_data.to_csv('test_data_2022_2024_NQ100.csv')\n",
    "\n",
    "# Print sample output to verify format\n",
    "print(\"\\nSample of training data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Print shape information\n",
    "print(\"\\nDataset shapes:\")\n",
    "print(f\"Train data: {train_data.shape}\")\n",
    "print(f\"Validation data: {valid_data.shape}\")\n",
    "print(f\"Test data: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to .pkl files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Save each set as .pkl files\n",
    "# locally\n",
    "train_path = '../data/NQ100/NQ100_dl_train.pkl'\n",
    "valid_path = '../data/NQ100/NQ100_dl_valid.pkl'\n",
    "test_path = '../data/NQ100/NQ100_dl_test.pkl'\n",
    "\n",
    "# google drive for colab\n",
    "train_path = '/content/drive/My Drive/ECE 570/data/NQ100/NQ100_dl_train.pkl'\n",
    "valid_path = '/content/drive/My Drive/ECE 570/data/NQ100/NQ100_dl_valid.pkl'\n",
    "test_path = '/content/drive/My Drive/ECE 570/data/NQ100/NQ100_dl_test.pkl'\n",
    "\n",
    "# Save train data\n",
    "with open(train_path, 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "# Save validation data\n",
    "with open(valid_path, 'wb') as f:\n",
    "    pickle.dump(valid_data, f)\n",
    "\n",
    "# Save test data\n",
    "with open(test_path, 'wb') as f:\n",
    "    pickle.dump(test_data, f)\n",
    "\n",
    "# Return the paths (optional)\n",
    "train_path, valid_path, test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ensure pkl files have content "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and test if data exists in each .pkl file\n",
    "def test_data_in_pkl(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        # Check if the DataFrame is not empty\n",
    "        if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "            print(f\"Data exists in {file_path} and has {len(data)} rows and {len(data.columns)} columns.\")\n",
    "        else:\n",
    "            print(f\"File {file_path} is either empty or not a DataFrame.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# Test each .pkl file\n",
    "test_data_in_pkl(train_path)\n",
    "test_data_in_pkl(valid_path)\n",
    "test_data_in_pkl(test_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
